\documentclass[specialist, 12pt, 
subf, %draft,
href, colorlinks=true,
substylefile = spbu.rtx,
]{disser}

%draft
\usepackage[
a4paper, mag=1000, includefoot,
left=3cm, right=1.5cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm
]{geometry}

\usepackage{float}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\ifpdf\usepackage{epstopdf}\fi
\usepackage{cmap}
\usepackage{graphicx}
\usepackage{multicol,caption}
\usepackage[final]{listings}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{wrapfig}
%\usepackage{subfigure}
%\usepackage{longtable}
\usepackage{array}
%\usepackage{bbold}
\usepackage{subcaption}
\setcounter{MaxMatrixCols}{48}
\usepackage{resizegather}
\usepackage{hyperref}
\let\vec=\mathbf
\widowpenalty=10000
\clubpenalty=10000
\setcounter{tocdepth}{2}


\graphicspath{{fig/}}


\newtheorem{proposition}{Предложение} 
\newtheorem{definition}{Определение} 
\newtheorem{theorem}{Теорема} 
\newtheorem{investigation}{Следствие} 
\newtheorem{designation}{Обозначение} 
\newtheorem{remark}{Замечание}
\newtheorem{example}{Пример}
\begin{document} 
	
	\lstset{ 
		basicstyle=\ttfamily\footnotesize, % the size of the fonts that are used for the code 
		numbers=left, % where to put the line-numbers 
		numberstyle=\footnotesize, % the size of the fonts that are used for the line-numbers 
		stepnumber=1, % the step between two line-numbers. If it's 1, each line 
		% will be numbered 
		numbersep=8pt, % how far the line-numbers are from the code 
		language=R} 
	\institution{ 
		Санкт-Петербургский государственный университет \\ 
		Прикладная математика и информатика \\ 
		Статистическое моделирование
	} 
	
	
	\title{Лекции Голяндиной Нины Эдуардовны} 
	
	% Тема 
	\topic{\normalfont\scshape % 
	Многомерный анализ данных} 
	
	% Автор 
	\author{Третьякова Александра, Зенкова Наталья} 
	

	
	% Город и год 
	\city{Санкт-Петербург}
	\date{2019}
	
	
	
	\maketitle
	\tableofcontents
	
	\intro
	\section{Генеральный язык} \label{q2}
Пусть случайная величина $\xi = (\xi_1, \ldots, \xi_p)^{\mathrm{T}} \in \mathbb{R}^p$, где $p$ --- количество признаков. Обозначим
\begin{itemize}
	\item \textit{вектор средних} $\mu = (\mathrm{E}\xi_1, \ldots, \mathrm{E}\xi_p)^{\mathrm{T}} \in \mathbb{R}^p$;
	\item \textit{ковариационную матрицу} $\Sigma = \mathrm{Cov} \xi = \mathrm{E}(\xi - \mathrm{E} \xi) (\xi - \mathrm{E} \xi)^{\mathrm{T}} \in \mathbb{R}^{p \times p}$.
\end{itemize}

Рассмотрим два случая:
\begin{itemize}
	\item \textit{Центрированные данные}: $\xi^{(c)} = \xi - \mathrm{E} \xi$. Тогда ковариационная матрица будет иметь вид: $\mathrm{Cov}\xi^{(c)} = \mathrm{E}\xi^{(c)}(\xi^{(c)})^{\mathrm{T}}$.
	\item \textit{Стандартизованные данные}: $\xi_i^{(s)} = \frac{\xi_i - \mathrm{E}\xi_i}{\sqrt{\mathrm{D}\xi_i}}$.\footnote{$\xi_i$ --- $i$-ая компонента вектора $\xi$.}\\ Запишем теперь это в матричном виде:
	 введём $\Delta$ --- диагональную матрицу, состоящую из элементов $\sqrt{\mathrm{D} \xi_i}$ для $i = 1, \ldots, p$. Тогда $\xi^{(s)} = \Delta^{-1} \xi^{(c)} \Rightarrow \mathrm{Corr} (\xi) = \mathrm{Cov} (\xi^{(s)})$, то есть $\mathrm{Cov}(\xi^{(s)})$ --- это корреляционная матрица до стандартизации.  
\end{itemize}

\section{Выборочный язык}
Перейдём теперь на выборочный язык. Пусть $\mathbf{x}_1, \ldots, \mathbf{x}_n \in \mathbb{R}^p$~---~выборка объёма $n$. Можем задать эмпирическое распределение как $\{\mathbf{x}_i \text{ с вероятностью } 1/n\}$. Возникает вопрос: как задать \textit{матрицу данных}? Удобно было бы ее задать как матрицу размера $p \times n$:
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		&          &                \\
		$\mathbf{x}_1$ & $\ldots$ & $\mathbf{x}_n$ \\
		&          &   \\
		\hline            
	\end{tabular}.
\end{table}
Но в книгах встречается следующий вариант \textit{матрицы данных}: 
$$\mathbb{X} = \begin{pmatrix}
\mathbf{x}_1^{\mathrm{T}}\\
\vdots\\
\mathbf{x}_n^{\mathrm{T}}
\end{pmatrix} = 
[X_1: \ldots: X_p] \in \mathbb{R}^{n \times p}.$$
Теперь знаем, как написать все характеристики, упомянутые выше, на выборочном языке. 
\begin{itemize}
	\item $\overline{x}_j = \frac{1}{n}\sum \limits_{i = 1}^{n} (X_j)_i$ --- \textit{выборочное среднее}, $j=1,\ldots,p$.
	
	 $\overline{X}_j=(\overline{x}_j,\ldots,\overline{x}_j)^\mathrm{T} \in \mathbb{R}^n.$
	
	
	\textit{Центрированная матрица данных} $\mathbb{X}^{(c)}$ \footnote{Посчитали среднее по каждому признаку и вычли. Таким образом, среднее по каждому признаку нулевое.}: 
	\begin{gather*}
	\mathbb{X}^{(c)} = [X_1 - \overline{X}_1: \ldots: X_p - \overline{X}_p]=[X_1^{(c)}:\ldots: X_p^{(c)}].
	\end{gather*}
	\item $s^2_j = \frac{1}{n} || X_j^{(c)}||^2$ --- \textit{выборочная дисперсия}.
	
	\textit{Матрица из стандартизованных признаков} $\mathbb{X}^{(s)}$:
	\begin{gather*}
	X_j^{(s)} = \frac{X_j - \overline{X}_j}{s_j} = \frac{X_j^{(c)}}{s_j}, \hspace{0.64 cm} \frac{1}{n}||X_j^{(s)}|| = 1 \hspace{0.4 cm} \forall \hspace{0.1 cm} j=1,\ldots, p.
	\end{gather*}
	
	\item $\mathbb{S} = \frac{1}{n}(\mathbb{X}^{(c)})^{\mathrm{T}}\mathbb{X}^{(c)}$ --- \textit{выборочная ковариационная матрица}.
	\item $\widehat{\mathrm{Corr}} \xi = \frac{1}{n}(\mathbb{X}^{(s)})^{\mathrm{T}}\mathbb{X}^{(s)}$ --- \textit{выборочная корреляционная матрица}.
\end{itemize}
\section{Примеры данных до стандартизации и после}
Зададимся вопросом, как выглядят данные до стандартизации и после неё? \\
\begin{enumerate}
	\item \textit{Данные с ненулевой корреляцией:}\footnote{Давайте подумаем, как будет выглядеть линия регрессии в первом и во втором случаях. Очевидно, что во втором случае она будет проходить через точку (0;0) и совпадать с прямой $y=x$.}
	
	\begin{minipage}{0.51\linewidth}
			\centering
		\includegraphics[width=150pt, height=150pt]{p01}
		\captionof{figure}{Данные до стандартизации}
		
	\end{minipage}
	\begin{minipage}{0.51\linewidth}
			\centering
		\includegraphics[width=150pt, height=150pt]{p02}
		\captionof{figure}{Данные после стандартизации}
		\label{p02}
	\end{minipage}

\newpage
	\item \textit{Данные с нулевой корреляцией:}
	
	\begin{minipage}{0.51\linewidth}
			\centering
		\includegraphics[width=150pt, height=150pt]{p03}
		\captionof{figure}{Данные до стандартизации}
	\end{minipage}
	\begin{minipage}{0.51\linewidth}
			\centering
		\includegraphics[width=150pt, height=150pt]{p04}
		\captionof{figure}{Данные после стандартизации}
	\end{minipage}
\end{enumerate}

\section{Многомерное нормальное распределение}\label{q1}
Рассмотрим $\xi \sim \mathrm{N_p}(\mu, \Sigma)$, где $\mu = \mathrm{E} \xi$ --- среднее значение $\xi$, $\Sigma = \mathrm{Cov} \xi$ --- ковариационная матрица. Предположим, что $\Sigma$ --- невырожденная матрица.\footnote{$|\Sigma| \neq 0$.} \textit{Плотность многомерного нормального распределения} задаётся формулой:
\begin{gather*}
\mathrm{p}(\mathbf{x}, \mu, \Sigma) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp \{-\frac{1}{2} (\mathbf{x} - \mu)^{\mathrm{T}}\Sigma^{-1}(\mathbf{x} - \mu)\} \text{ для } \forall \mathbf{x}  \in\mathbb{R}^{p}, \text{ где}
\end{gather*}
$p$ --- количество признаков. Очевидно, что в одномерном случае плотность будет иметь следующий вид:
\begin{gather*}
p(x) = \frac{1}{\sqrt{2 \pi} \sigma} \exp \{-\frac{(x - \mu)^2}{2\sigma^2}\}, \text{ где}
\end{gather*}
$\mu$, $\sigma$ --- числа.
Введём следующее понятие --- \textit{расстояние Махаланобиса}:
\begin{gather*}
r^2_M(\mathbf{x}, \mu, \Sigma) = (\mathbf{x} - \mu)^{\mathrm{T}} \Sigma^{-1} (\mathbf{x} - \mu). 
\end{gather*}
Отметим, что $\Sigma$ в данном случае является неотрицательно определённой симметричной матрицей. Но так как ковариационная матрица удовлетворяет данному условию, то получаем, что если расстояния Махаланобиса совпадают, то и плотности многомерного нормального распределения тоже. Посмотрим, что будет являться расстоянием Махаланобиса в одномерном случае ($p = 1$):
\begin{gather*}
\sqrt{\frac{(x - \mu)^2}{\sigma^2}} = \frac{|x - \mu|}{\sigma}=r_M(x,\mu,\sigma^2) \text{ --- \textit{расстояние до центра, измеренное в $\sigma$}}.
\end{gather*}
\textbf{На выборочном языке:} $$\frac{|x_i - \overline{x}|}{s_i}.$$
Таким образом, если у нас есть нормальное распределение, то в одномерном случае мы можем измерять расстояние в сигмах, а в случае многомерного нормального распределения мы смотрим на линии уровня (точки, находящиеся на одном расстоянии Махаланобиса от центра).
\begin{center}
	\begin{minipage}{0.51\linewidth}
		\centering
		\includegraphics[width=150pt, height=150pt]{p05}
		\captionof{figure}{Линии уровня}
	\end{minipage}
\end{center}


Ответим на следующий вопрос: \textit{какое преобразование необходимо сделать со случайной величиной, имеющей нормальное распределение, чтобы из зависимости получить независимость?} 
\begin{center}
	\begin{minipage}{0.8\linewidth}
		\centering
		\includegraphics[width=150pt, height=150pt]{p06}
		\captionof{figure}{Отбеливание}
	\end{minipage}
\end{center}

Сначала делаем поворот, а затем стандартизуем. Такое преобразование еще называется "отбеливанием".

Пусть $\zeta = \Sigma^{-1/2} (\xi - \mu)$ --- новая случайная величина, полученная из исходной линейным преобразованием. Можем считать, что $\mathrm{E}\zeta = \mathbb{O}$, так как $$\mathrm{E} \zeta = \Sigma^{-1/2} (\mathrm{E} \xi - \mu) = \mathbb{O}.$$
Посчитаем ковариацию $\zeta$:\footnote{В данном доказательстве воспользовались симметричностью и неотрицательной определённостью ковариационной матрицы $\Sigma$.}
\begin{gather*}
\text{Cov} (\zeta) = \mathrm{E} \zeta \zeta^{\mathrm{T}} =\Sigma^{-1/2} \text{Cov} (\xi) \Sigma^{-1/2} = \Sigma^{-1/2} \underbrace{\Sigma^1}_{\mathrm{E}\xi \xi^{\mathrm{T}}} \Sigma^{-1/2} = \mathbb{I}.
\end{gather*}


Вспомним несколько свойств, которые характерны для нормально распределённых случайных величин: 
\begin{enumerate}
	\item Если $\xi \sim \mathrm{N}$, то и всякое линейное преобразование $\xi$ тоже имеет нормальное распределение. 
	\item Знаем, что из независимости следует некоррелированность. В случае нормального распределения верно и обратное.
	В общем случае это неверно. Приведём пример, когда зависимость есть, но корреляция равна 0. 
	\begin{center}
		\begin{minipage}{0.51\linewidth}
			\centering
			\includegraphics[width=150pt, height=150pt]{p07}
			\captionof{figure}{Пример данных, когда зависимость есть, но корреляция нулевая}
		\end{minipage}
	\end{center}
	\item Условное математическое ожидание является линейной функцией.
	\item Ковариационная матрица $\Sigma$ может быть вырожденной:\footnote{Наблюдения лежат на одной прямой.}
	\begin{center}
		\begin{minipage}{0.4\linewidth}
			\centering
			\includegraphics[width=150pt, height=150pt]{p08}
			\captionof{figure}{Ковариационная матрица вырождена}
		\end{minipage}
	\end{center}
	
\end{enumerate}

\section{Переход к новым признакам}
Пусть $\xi$ --- $p$-мерная случайная величина.\footnote{$p$ --- количество признаков.} Считаем, что среднее равно нулю. Хотим образовать новый признак, который является линейной комбинацией старых признаков.\footnote{Новый признак будет в некотором смысле лучше, чем старые.}
\begin{gather*}
\eta = \mathbf{A}^{\mathrm{T}}\xi = \xi^{\mathrm{T}} \mathbf{A} = a_1 \xi_1 + \ldots + a_p \xi_p, \hspace{0.3 cm} \mathbf{A} \in \mathbb{R}^p,\\
\mathrm{D}\eta = \mathbf{A}^{\mathrm{T}} \mathbf{\Sigma} \mathbf{A}.
\end{gather*}
Теперь пусть $\eta = (\eta_1, \ldots, \eta_d)^{\mathrm{T}} \in \mathbb{R}^d$ --- $d$-мерная случайная величина,\footnote{d новых признаков.} которая получается из исходной $\eta = \mathbb{A}^{\mathrm{T}} \xi$, где $\mathbb{A} \in \mathbb{R}^{p\times d}$. Соответствующая ковариационная матрица будет иметь вид $\mathrm{Cov} (\eta) = \mathbb{A}^{\mathrm{T}} \mathbf{\Sigma} \mathbb{A}$.

\textbf{На выборочном языке:} $Z = \sum\limits_{j = 1}^{p} a_j X_j = \mathbb{X} A \in \mathbb{R}^n$ или $\mathbb{Z} = \mathbb{X} \mathbb{A} \in \mathbb{R}^{p \times d}$.\footnote{$\mathbb{X} \in \mathbb{R}^{n \times p}$ --- матрица данных.} \\
~\\

Приведем примеры перехода к новым признакам.
\begin{enumerate}
	\item Возьмём в качестве индивида школьников, а в качестве признаков --- оценки по четырём школьным предметам:
	\begin{itemize}
		\item $X_1$ --- оценка по математике;
		\item $X_2$ --- оценка по физике;
		\item $X_3$ --- оценка по русскому;
		\item $X_4$ --- оценка по литературе;
	\end{itemize}
Сами по себе оценки мало несут информации про общее положение в школе или классе, поэтому давайте создадим новые признаки, которые будут более информативными. Пусть это будут
\begin{itemize}
	\item $Z_1 = X_1 + X_2 + X_3 + X_4$ --- сумма оценок по всем предметам;
	\item $Z_2 = X_1 + X_2 - (X_3 + X_4)$ --- разность между оценками по естественным и гуманитарным наукам.
\end{itemize}
Выясним, как будет выглядеть матрица $\mathbb{A}$ в этом случае:
\begin{gather*}
\mathbb{A} = \begin{pmatrix}
1 & 1\\
1 & 1\\
1 & -1\\
1 & -1
\end{pmatrix} \in \mathbb{R}^{4 \times 2}.
\end{gather*}

\item Рассмотрим два признака: оценки по математике $X_1$ и физике $X_2$.

	\begin{center}
	\begin{minipage}{0.51\linewidth}
		\centering
		\includegraphics[width=150pt, height=170pt]{p10}
		\captionof{figure}{Scatterplot: $X_1$ and $X_2$}
		\label{10}
	\end{minipage}
\end{center}

В данной ситуации получается, что новый признак $X_1 + X_2$ лучше всего характеризует данные, то есть это та характеристика, по которой ученики максимально различаются.\footnote{$X_1 - X_2$ --- характеристика, по которой ученики минимально отличаются друг от друга.}\footnote{Необходимо выбрать признак, который бы наилучшим образом характеризовал данные. Такой характеристикой будет тот признак, по которому данные максимально различаются. Таким образом, приходим к анализу главных компонент.}

\item  Рассмотрим два признака: оценки по математике $X_1$ и литературе $X_4$. Максимальное отличие получаем по разности данных признаков.	
\begin{center}
	\begin{minipage}{0.51\linewidth}
		\centering
		\includegraphics[width=150pt, height=170pt]{p11}
		\captionof{figure}{Scatterplot: $X_1$ and $X_4$}
		\label{10}
	\end{minipage}
\end{center}
\end{enumerate}

\subsection{Две интерпретации новых признаков. Факторные нагрузки}
Обсудили, зачем необходимы новые признаки. Также до этого мы требовали, чтобы новые признаки были линейной комбинацией старых. Теперь предположим, что:
\begin{itemize}
	\item на выборочном языке: $z_i \perp z_j$;\footnote{Признаки ортогональны.}
	\item на генеральном языке: $\rho(\eta_i, \eta_j) = 0$.\footnote{Признаки хотя бы некоррелированы.}
\end{itemize}  
Зависимость признаков означает, что кусок одного признака входит в другой, то есть получается некоторая избыточность.

Пусть $\mathrm{rank}(\mathbb{X}) = d$, $(z_1, \ldots, z_d)$ --- ортогональный базис в в подпространстве размерности $d$ --- $\mathrm{span}(X_1, \ldots, X_p) = \mathrm{colspan}(\mathbb{X})$.
Превратим базис в \textit{ортонормированный базис}: $Q_i = \frac{z_i}{||z_i||}$.\footnote{Рассмотрим пример: пространство $\mathbb{R}^2$, соответствующий базис $(1,1)^{\mathrm{T}}/\sqrt{2}$, $(1,-1)^{\mathrm{T}}/\sqrt{2}$. Это ортонормированный базис. Как вычислить координаты вектора $(5, 4)^{\mathrm{T}}$ в данном базисе?\\
Если $U_1, \ldots, U_d$ --- ортогональный базис в $\mathbb{R}^d$, тогда $\forall$ $A \in \mathbb{R}^d$ раскладывается по ортонормированному базису: 
\begin{gather*}
A = \sum\limits_{i = 1}^d <A, U_i> U_i, \text{ где $<A, U_i>$ --- $i$-ая координата вектора $A$ в базисе $\{U_j\}_{j = 1}^d$.}
\end{gather*}
}
 Таким образом, $\{Q_i\}_{i=1}^d$ --- ортонормированный базис в пространстве $\mathrm{span}(X_1, \ldots, X_p)$.


\textit{Новые признаки на самом деле интерпретируются двумя способами:}
\begin{itemize}
	\item Новые признаки задаются равенством:\footnote{Новые признаки есть линейная комбинация старых.}
	\begin{gather*}
	\mathbb{X}\mathbb{A} = \mathbb{Z}.
	\end{gather*}
	\item Исходные признаки выражаются через новые:\footnote{Есть пространство, в нём базис, каждый вектор этого пространства раскладывается по базису.}
	\begin{gather}\label{1}
	\forall \hspace{0.1 cm} i = 1,\ldots,p \hspace{0.4 cm} X_i = \sum\limits_{j = 1}^d <X_i, Q_j> Q_j.
	\end{gather} 
\end{itemize}
\begin{definition}
	$f_{ij} = <X_i, Q_j>$ --- факторные веса, факторные нагрузки.
\end{definition}

Составим матрицу $\mathbb{F} = \{f_{ij}\}_{i = 1, j = 1}^{p,d}= [F_1,:\ldots:F_d] \in \mathbb{R}^{p \times d}$ --- коэффициенты разложения по ортонормированному базису, тогда можем записать разложение \ref{1} в матричном виде:
\begin{gather}\label{2}
\mathbb{X} = \sum\limits_{j = 1}^d Q_j F_j^{\mathrm{T}} = \mathbb{Q}\mathbb{F}^{\mathrm{T}}.
\end{gather}
$||Q_j|| = 1$, $||F_j|| \neq 1$ в разложении \ref{2}. Давайте нормируем $F$. Пусть $\sigma_j = ||F_j||$, $P_j = \frac{F_j}{||F_j||}$, тогда $||Q_j|| = ||P_j|| = 1$ и
\begin{gather}\label{3}
\mathbb{X} = \sum\limits_{j = 1}^d \sigma_j Q_j P_j^{\mathrm{T}} = \mathbb{Q} \mathbf{\Sigma} \mathbb{P}^{\mathrm{T}}, \text{ где $\mathbf{\Sigma} = \mathrm{diag}\{\sigma_1, \ldots, \sigma_d\}$.}
\end{gather}
\textit{Что за матрица $Q_jP_j^{\mathrm{T}} \in \mathbb{R}^{n \times p}$?} $\mathrm{rank}(Q_jP_j^{\mathrm{T}}) = 1$.\footnote{Столбцы пропорциональны.} Таким образом, у нас была исходная матрица ранга $d$, а мы превратили её в сумму $d$ элементарных матриц ранга 1.

%\begin{proposition}
%	$\mathbf{\Sigma}\mathbb{P}^{\mathrm{T}} = \mathbb{F}^{\mathrm{T}}$.
%\end{proposition}
%\begin{proof}[Доказательство.]
%	\textcolor{red}{И это бы надо бы пояснить}
%\end{proof}

Пусть $\mathbb{X}^{(j)} = \sigma_jQ_jP_j^{\mathrm{T}}$, тогда $\mathbb{X} = \sum\limits_{j = 1}^d \mathbb{X}^{(j)}$.
\subsection{Вклад новых признаков}
Ввели новый признак, теперь хотелось бы понять, какую важную часть информации он в себя включает. Введём скалярное произведение двух матриц:\footnote{Чтобы уметь измерять вклад признака, необходимо уметь измерять одним числом, то есть ввести норму матрицы. Норма по Фробениусу: $||\mathbb{A}||^2_F = \sum\limits_{ij}a_{ij}^2$.}
\begin{gather*}
(\mathbb{Y}, \mathbb{Z})_F = \sum\limits_{i, j} y_{ij} z_{ij}.
\end{gather*}

Если верно $||\mathbb{X}||^2 = ||\sum\limits_{j = 1}^d \mathbb{X}^{(j)}||^2_F \stackrel{?}{=} \sum\limits_{j = 1}^d||\mathbb{X}^{(j)}||^2_F$, то \textit{вклад $j$-ого признака} (вклад $j$-ой элементарной матрицы) --- $\frac{||\mathbb{X}^{(j)}||^2}{||\mathbb{X}||^2}$.\footnote{Если отношение равно $1/2$, то значит, что $j$-ый признак измеряет $50\%$ всей информации.}
\begin{remark}
	$||\sum\limits_{j = 1}^d \mathbb{X}^{(j)}||^2_F = \sum\limits_{j = 1}^d||\mathbb{X}^{(j)}||^2_F$ (скалярное произведение равно 0, потому что $Q_i \perp Q_j$).
\end{remark}

Так как $Q_j$ нормированы, то можем продолжить равенство: 
\begin{gather*}
||\mathbb{X}||^2 = ||\sum\limits_{j = 1}^d \mathbb{X}^{(j)}||^2_F = \sum\limits_{j = 1}^d||\mathbb{X}^{(j)}||^2_F = \sum\limits_{j = 1}^d \sigma_j^2.
\end{gather*}

\begin{remark}
	Пусть старые признаки $X_i$ центрированные, тогда новые признаки $Z_i$ и $Q_i$ тоже центрированные.
\end{remark}
\begin{proof}[Доказательство.]
	 Новые признаки --- это линейная комбинация старых. Среднее линейной комбинации есть линейная комбинация средних по признакам. Средние по признакам равны 0, получаем центрированные новые признаки.
\end{proof}

\section{Пара фактов из линейной алгебры}
\begin{enumerate}
	\item \textit{Унитарная матрица} $\mathbb{U}$ --- ортогональная матрица в комплексном случае.
	\begin{itemize}
		\item $\mathbb{U}$ --- квадратная матрица: $\mathbb{U}^{\mathrm{T}} =\mathbb{U}^{-1}$.
		\item Столбцы $\mathbb{U}$ ортонормированы. 
		\item Строки $\mathbb{U}$ ортонормированы.\footnote{2 пункт эквивалентен 3. \textit{Почему?} Если матрица ортогонональная, то и транспонированная к ней тоже ортогональная (следует из пункта 1).}
	\end{itemize}

Умножение на матрицу $\mathbb{U}$ означает \textit{поворот} или \textit{отражение}. Пусть есть вектора $Y$, $Z$, после умножения на матрицу $\mathbb{U}$ получим $\widetilde{Y} = \mathbb{U}Y,~ \widetilde{Z}=\mathbb{U}Z$.\footnote{Поворачиваем вектора $Y$ и $Z$ на какой-то угол, а также при домножении на матрицу $\mathbb{U}$ не меняются нормы векторов.}
\begin{example}
	 $\mathbb{U} = \begin{pmatrix}
	\cos \phi & \sin \phi\\
	-\sin \phi & \cos \phi
	\end{pmatrix}$.
\end{example}
\item Пусть $\{P_i\}_{i = 1}^r$ --- система независимых векторов, рассмотрим линейную оболочку $\mathcal{L}_r = \mathrm{span}\{P_1, \ldots, P_r\}$ в $\mathbb{R}^L$, $\Pi: \mathbb{R}^L \rightarrow \mathcal{L}_r$ --- проектор на $\mathcal{L}_r$. Матрица $\Pi$: 
\begin{gather*}
\Pi = \mathbb{P} (\mathbb{P}^{\mathrm{T}} \mathbb{P})^{-1} \mathbb{P}^{\mathrm{T}}.
\end{gather*}
Пусть $\{P_i\}_{i = 1}^r$ --- ортонормированный базис $\mathcal{L}_r$, тогда
\begin{gather*}
\mathbb{P}^{\mathrm{T}}\mathbb{P} = \mathbb{I}_{r\times r} =  \begin{pmatrix}
1 & \cdots & 0\\
& \ddots & \\
0 & \cdots & 1
\end{pmatrix} \in \mathbb{R}^{r \times r} \text{ и } \Pi = \mathbb{P} \mathbb{P}^{\mathrm{T}}.
\end{gather*} 
\end{enumerate}

\chapter{Сингулярное разложение\\ (SVD --- Singular Value Decomposition)}
\section{Как строится сингулярное разложение}
Пусть $L$ --- число признаков, $K$ --- количество индивидов, $\mathbb{Y} = \mathbb{X}^{\mathrm{T}} \in \mathbb{R}^{L \times K}$ --- ненулевая матрица. Обозначим $\mathbb{S} = \mathbb{Y}\mathbb{Y}^{\mathrm{T}} \in \mathbb{R}^{L \times L}$ --- симметричная неотрицательно определённая матрица.
\begin{gather*}
\mathbb{S} U_i = \lambda_i U_i, \text{ где}
\end{gather*} 
$\{U_i\}_{i = 1}^L$ --- ортонормированный набор из собственных векторов матрицы $\mathbb{S}$, \\
$\lambda_1 \geq \ldots \geq \lambda_L \geq 0$ --- собственные числа матрицы $\mathbb{S}$.\footnote{Положительные, т.к. матрица $\mathbb{S}$ неотрицательно определена.}


Пусть $d = \mathrm{rank} \mathbb{Y} = \mathrm{colrank} \mathbb{Y} = \mathrm{rowrank}\mathbb{Y}$. Знаем, что $d \leq \min (L,K)$.
\begin{proposition}
\begin{enumerate}
	\item $d = \mathrm{rank} \mathbb{Y} \mathbb{Y}^{\mathrm{T}}$.
	\item $\lambda_d > 0$; $\lambda_i = 0$ при $i > d$.\footnote{Упорядочили собственные числа: первые $d$ строго положительные, а остальные все нули.}
	\item $\{U_i\}_{i = 1}^d$ образуют ортонормированный базис $\mathrm{colspan} \mathbb{Y}$.
\end{enumerate}	
\end{proposition}

Введём вектор
\begin{gather*}
V_i \stackrel{def}{=} \frac{\mathbb{Y}^{\mathrm{T}}U_i}{\sqrt{\lambda_i}} \in \mathbb{R}^k, \hspace{0.2 cm} i = 1, \ldots, d.
\end{gather*}
\begin{proposition}
	\begin{enumerate}
		\item $\{V_i\}_{i = 1}^d$ --- оронормированная система векторов.
		\item $V_i$ --- собственные вектора $ \mathbb{Y}^{\mathrm{T}}\mathbb{Y}$, соответствующие тем же собственным числам $\lambda_i$. Остальные собственные вектора $ \mathbb{Y}^{\mathrm{T}}\mathbb{Y}$ соответствуют нулевым собственным числам.
		\item $U_i = \frac{\mathbb{Y}V_i}{\sqrt{\lambda_i}}$.
		\item $\mathbb{Y} = \sum\limits_{i = 1}^d \sqrt{\lambda_i} U_i V_i^{\mathrm{T}}$ --- SVD (Сингулярное разложение матрицы).\footnote{Разложение в сумму элементарных матриц.}\footnote{Самый важный пункт утверждения.}
	\end{enumerate}
\end{proposition}
\textit{Что здесь считать новыми признаками, если $\mathbb{Y} = \mathbb{X}^{\mathrm{T}}$}? $V_i$,\footnote{Они длинные :)} так как $U_i \in \mathbb{R}^L$, $V_i \in \mathbb{R}^K$.
\begin{itemize}
	\item $U_i$ --- ортонормированный базис в пространстве столбцов.
	\item $V_i$ --- ортонормированный базис в пространстве строк.\footnote{Можем $\mathbb{X}$ транспонировать, проделать всё то же самое, а поменяются местами только $U_i$ и $V_i$.}
	\item $\frac{\lambda_i}{\sum_{i} \lambda_i}$ --- вклад $i$-ого признака.
\end{itemize}
 \begin{definition}
 		$\sqrt{\lambda_i}$ --- \textit{сингулярные числа} матрицы $\mathbb{Y}$, $U_i$ --- \textit{левый сингулярный вектор}, $V_i$ --- \textit{правый сингулярный вектор}.
 		
 	Тройка $(\sqrt{\lambda_i},U_i,V_i)$ называется $i$-ой собственной тройкой сингулярного разложения.
 \end{definition}

\begin{remark}
	Сингулярное разложение --- единственное разложение с двумя ортонормированными базисами.
\end{remark}

\section{Матричный вид сингулярного разложения}
Можно записать двумя способами:
\begin{enumerate}
	\item Введём $\mathbb{U}_d = [U_1: \ldots: U_d]$, $\mathbb{V}_d = [V_1: \ldots: V_d]$, $\Lambda_d = \mathrm{diag}(\lambda_1, \ldots, \lambda_d)$. Тогда 
	\begin{gather*}
	\mathbb{Y} = \mathbb{U}_d\Lambda_d^{1/2}\mathbb{V}_d^{\mathrm{T}}.
	\end{gather*}
	\item Возьмём $\mathbb{U} = [U_1: \ldots: U_d: U_{d+1}: \ldots: U_L]$ --- ортонормированный базис в $\mathbb{R}^L$.\footnote{$U_{d+1}, \ldots, U_L$ соответствуют нулевому собственному числу матрицы.}\\
	$\mathbb{V}^\mathrm{T} = [V_1: \ldots: V_d: V_{d+1}: \ldots: V_K]$ --- ортонормированный базис в $\mathbb{R}^K$.\\
	$\Lambda = \begin{pmatrix}
	\lambda_1 & 0 & \ldots & \ldots & 0\\
	0 & \ddots & & 0 & 0\\
	0 & & \lambda_d & & 0&\\
	0 & & & \ddots & 0\\
	0 & 0 & \ldots & & 0 &
	\end{pmatrix} \in \mathbb{R}^{L \times K}$. Тогда\footnote{$\mathbb{U}, \mathbb{V}$ --- ортогональные матрицы.}
	\begin{gather*}
	\mathbb{Y} = \mathbb{U} \Lambda^{1/2} \mathbb{V}^{\mathrm{T}}.
	\end{gather*}
	~\\
\end{enumerate}
\section{Единственность сингулярного разложения}
\textit{Насколько единственно разложение SVD (оно одно существует для матрицы или нет)}?
Можно подумать, что разложение не единственное, так как 
\begin{enumerate}
	\item Собственные вектора не единственные, то есть если $U_i$ --- собственный вектор, то $-U_i$ --- собственный вектор.\footnote{В вещественном случае собственный вектор определяется единственным образом с точностью до константы, модуль которой равен 1 (это всего 1 и -1). Но сингулярное разложение обобщается в комплексном случае, а здесь уже констант, по модулю равных 1, много.}
	\begin{gather*}
	\mathbb{Y} = \sum\limits_{i = 1}^d \sqrt{\lambda_i}U_i V_i^{\mathrm{T}} = \sum\limits_{i = 1}^d \sqrt{\lambda_i}(-U_i) (-V_i)^{\mathrm{T}}.
	\end{gather*}
	\item Пусть есть два одинаковых собственных числа $\lambda = \lambda_1 = \lambda_2$, $U_1$ и $U_2$ --- два ортонормированных вектора, соответствующих собственному числу $\lambda$. Тогда любая линейная комбинация $U_1$ и $U_2$ будет являться также собственным вектором и будет соответствовать тому же собственному числу, то есть $\forall$ $\alpha, \beta$: $\alpha U_1 + \beta U_2$ --- с.в. с с.ч. $\lambda$. Таким образом, если у нас есть два одинаковых собственных числа, то они порождают подпространство размерности 2, и любой ортонормированный базис в этом подпространстве подходит нам в качестве собственного вектора.\footnote{Если у нас есть два одинаковых собственных числа, то мы можем брать любой базис, но сумма двух матриц постоянна, то есть она не меняется от выбора базиса.  }
\end{enumerate}

Получаем, что единственности в буквальном смысле не получается. Поэтому сформулируем необходимое нам утверждение.
\begin{proposition}[Единственность SVD]
	Пусть $L\le K$. Пусть $\mathbb{Y} = \sum\limits_{i = 1}^L c_i P_i Q_i^{\mathrm{T}}$ --- некоторое разложение в сумму элементарных матриц (биортогональное разложение), такое что:
	\begin{enumerate}
		\item $c_1 \geq \ldots \geq c_L \geq 0$;
		\item $\{P_i\}_{i=1}^L$ --- ортонормированные, $\{Q_i\}_{i=1}^L$ --- ортонормированные.
	\end{enumerate}
Тогда $\mathbb{Y} = \sum\limits_{i = 1}^L c_i P_i Q_i^{\mathrm{T}}$ --- SVD, то есть \textit{любое биортогональное разложение c неотрицательными коэффициентами является сингулярным.}
\end{proposition}
~\\

\begin{remark}
	В частности:
	\begin{itemize}
	\item $c_d > 0$, $c_{d+1} = \ldots = c_L = 0$,
	\item $c_i^2 = \lambda_i$ --- собственные числа $\mathbb{Y}\mathbb{Y}^{\mathrm{T}}$,
	\item $P_i$ --- собственные вектора $\mathbb{Y}\mathbb{Y}^{\mathrm{T}}$,
	\item $Q_i$ --- собственные вектора $\mathbb{Y}^{\mathrm{T}}\mathbb{Y}$,
	\item $Q_i = \frac{\mathbb{Y}^{\mathrm{T}} P_i}{\sqrt{\lambda_i}}$, $i = 1,\ldots,d$ ($d = \mathrm{rank}\mathbb{Y}\mathbb{Y}^\mathrm{T}$).
	\end{itemize} 
\end{remark}

\section{Оптимальные свойства сингулярного разложения}
Обозначим $M_r \subset \mathbb{R}^{L\times K}$ --- пространство матриц ранга, меньшего или равного $r$.
\begin{proposition}[Оптимальные свойства сингулярного разложения]
	Пусть $r\le d$.
	\begin{enumerate}
		\item (Аппроксимация матрицей (Low-rank approximation)) 
		
		$\min \limits_{\widetilde{\mathbb{Y}}\in M_r} \Arrowvert \mathbb{Y} - \widetilde{\mathbb{Y}}\Arrowvert^2_F =\sum\limits_{i =r+1}^d \lambda_i $ и достигается на $\widetilde{\mathbb{Y}}=\sum\limits_{i =1}^r \sqrt \lambda_i U_i V_i^\mathrm{T}.$
		\item (Аппроксимация подпространством) 
		
		Пусть $\mathcal{L}_r \subset \mathbb{R}^L$ --- подпространство размерности $\le r$. Тогда 
		\begin{equation*}
		\min\limits_{\mathcal{L}_r} \sum \limits_{i =1}^K dist^2(Y_i,\mathcal{L}_r)=\sum \limits_{i =r+1}^d \lambda_i
		\end{equation*}
		и достигается на $\mathcal{L}_r^{(0)}=\text{span} (U_1,\ldots,U_r)$.
	\end{enumerate}
\end{proposition}

Попробуем ответить на вопрос --- \textit{что выгоднее хранить в памяти --- всю матрицу или ее сингулярное разложение?} Чтобы хранить матрицу данных размера $L\times K$, требуется хранить $LK$ элементов. Чтобы хранить вектора сингулярного разложения, требуется $d(L+K)$ элементов.\footnote{Всего $d$ сингулярных троек, $U_i \in \mathbb{R}^L$, $V_i\in \mathbb{R}^K$.} Таким образом, если, к примеру, матрица близка к квадратной ($L=K$), то при $L>2d$, выгоднее хранить сингулярное разложение.





\chapter{Анализ главных компонент (АГК)\\(PCA --- principal component analysis)}

\section{Главные направления}
Пусть $Y_1,\ldots,Y_K\in \mathbb{R}^L$. Рассмотрим вектор $P: \Arrowvert P \Arrowvert =1$. Этот вектор задает направление (прямую, подпространство размерности 1). Проекция на данное направление выглядит следующим образом: $\langle Y_i,P\rangle ^2$. Проекция измеряет то, насколько это направление соответствует нашим данным. 
Поставим задачу найти направление, которое лучше всего описывает нашу совокупность точек. Чем больше проекция, тем лучше. Задача:
\begin{equation*}
\sum\limits_{i =1}^K \langle Y_i,P\rangle ^2 \to \max\limits_P. 
\end{equation*}

Вектор $P_1$, на котором достигается максимум, называется \textit{первым главным направлением.}

Далее будем искать максимум по всевозможным векторам, ортогональным $P_1$ и так далее.

\begin{proposition}
	\begin{equation*}
	1. ~~ \max\limits_P \sum\limits_{i =1}^K \langle Y_i,P\rangle ^2 =\lambda_1 \text{ и достигается на } P=U_1,
	\end{equation*}

	\begin{equation*}
	2. ~~ \max\limits_{P: ~P\perp U_1} \sum\limits_{i =1}^K \langle Y_i,P\rangle ^2 =\lambda_2 \text{ и достигается на } P=U_2,
	\end{equation*}
	\begin{equation*}
    \ldots
	\end{equation*}
	\begin{equation*}
	r. ~~ \max\limits_{P:~ P\perp U_j,~ j=1,\ldots,r-1} \sum\limits_{i =1}^K \langle Y_i,P\rangle ^2 =\lambda_r \text{ и достигается на } P=U_r.
	\end{equation*}
\end{proposition}

$U_i$ --- \textit{главные направления}.

Разложение $Y_j$ по главным направлениям: $Y_j=\sum\limits_{i =1}^r \langle Y_j,U_i\rangle U_i.$

$\langle Y_j,U_i\rangle$ --- \textit{i-я компонента вектора $Y_j$} (коэффициент разложения вектора по главным направлениям). Составим \textit{вектор i-х главных компонент:}
\begin{equation*}
Z_i= \left( \begin{matrix}
\langle Y_1,U_i\rangle \\
\ldots\\
\langle Y_K,U_i\rangle\\
\end{matrix} \right) = \mathbb{Y}^\mathrm{T} U_i=\sqrt{\lambda_i} V_i=\mathbb{X}U_i.
\end{equation*}


\section{Анализ главных компонент. Построение}
\subsection{Разложение Гильберта-Шмидта}

Пусть есть множества $D_1 = \{1, \ldots, L\}$, $D_2 = \{1, \ldots, K\},$ $\mu_1,~\mu_2$ --- считающие меры  (то есть $\mu_1(\{i\})=1~~ \forall i=1,\ldots,L,$  $\mu_2(\{j\})=1 ~~\forall j=1,\ldots,K$).
Введем два гильбертовых пространства: $L_1=L^2(D_1,\mu_1)$ и $L_2=L^2(D_2,\mu_2)$ со скалярными произведениями $\langle \cdot,\cdot \rangle_1$ и  $\langle \cdot,\cdot \rangle_2$ и нормами $\Arrowvert \cdot \Arrowvert_1$ и $\Arrowvert \cdot \Arrowvert_2$ соответственно.  Заметим, что $L_1$ --- обычное пространство векторов длины $L$, а $L_2$ --- пространство векторов длины $K$ со стандартным евклидовым скалярным произведением. 

Можем ввести отображение $\mathcal{G}:L_2 \to L_1$. В наших обозначениях это отображение переводит вектор размерности $K$ в вектор размерности $L$. В качестве $\mathcal{G}$ можем брать отображение, которое задается умножением на матрицу $\mathbb{G} \in \mathbb{R}^{L\times K}$, то есть $\forall Y\in \mathbb{R}^K~~ \mathbb{G}Y=X\in\mathbb{R}^L$. 

Зададим сопряженное отображение $\mathcal{G}^*:L_1 \to L_2$ такое, что $\langle X, \mathbb{G}Y\rangle_1 = \langle \mathcal{G}^* X, Y \rangle _2$ $\forall~ X\in L_1,~Y\in L_2$. Тогда действие $\mathcal{G}^*$ --- это умножение на матрицу $\mathbb{G}^\mathrm{T}$.

Оператор $\mathcal{G}$ по определению задается ядром $g(x,s)$:
\begin{equation*}
(\mathcal{G}h)(x)=\int\limits_{D_2} g(x,s)h(s)\mu_2(ds).
\end{equation*}
При введенных нами $D_1, D_2$ $g(x,s)$ --- это просто элементы матрицы $\mathbb{G}$.

Далее рассмотрим операторы $\mathcal{G}\mathcal{G}^*:L_1\to L_1$ и $\mathcal{G}^*\mathcal{G}:L_2\to L_2$.
\begin{remark}
	Пусть меры не считающие, то есть $\mu_1(\{i\})=w_i~~ \forall i=1,\ldots,L$, а $\mu_2(\{j\})=q_i ~~\forall j=1,\ldots,K.$ Обозначим диагональные матрицы $\mathbb{W}=\text{diag}(w_i)$ и  $\mathbb{Q}=\text{diag}(q_i)$. Пусть оператор $\mathcal{G}$ задается умножением на матрицу $\mathbb{G}$. Тогда оператор $\mathcal{G}\mathcal{G}^*$ задается матрицей $\mathbb{G}\mathbb{Q}\mathbb{G}^\mathrm{T}\mathbb{W}$, а оператор $\mathcal{G}^*\mathcal{G}$ задается матрицей $\mathbb{G}^\mathrm{T}\mathbb{W}\mathbb{G}\mathbb{Q}$.
\end{remark}

Ядро оператора $\mathcal{G}\mathcal{G}^*$ выглядит следующим образом:

\begin{equation*}
g_{1,1}(x,s)=\int\limits_{D_2} g(x,s)g(y,s)\mu_2(ds).
\end{equation*}

Обозначим $\{\phi_n\},~\phi_n\in L_1$ --- ортонормированная система собственных функций оператора $\mathcal{G}\mathcal{G}^*$;  $\{\psi_n\},~\psi_n\in L_2$ --- ортонормированная система собственных функций оператора $\mathcal{G}^*\mathcal{G}$. Известно, что им соответствуют одни и те же собственные числа $\lambda_n$ и что $\psi_n=\frac{\mathcal{G}^*\phi_n}{\sqrt{\lambda_n}}$ и $\phi_n=\frac{\mathcal{G}\psi_n}{\sqrt{\lambda_n}}$. Обратим внимание, что для операторов $\mathcal{G}$, заданных одним и тем же ядром, собственные числа и вектора могут различаться, если меры в пространствах заданы разные. Разложение Шмидта ядра оператора $\mathcal{G}$:
\begin{equation*}
g(x,s)=\sum\limits_n\sqrt{\lambda_n}\phi_n(x)\psi_n(s)
\end{equation*}
\begin{remark}
	Если $g(i,j)$ --- элементы матрицы $\mathbb{G}\in\mathbb{R}^{L\times K}$, а меры считающие, тогда разложение Шмидта ядра оператора $\mathcal{G}$ --- это в точности сингулярное разложение матрицы $\mathbb{G}$.  
\end{remark}

\subsection{Анализ главных компонент на выборочном языке}

Вернемся к сингулярному разложению. Имеем разложение:
\begin{gather*}
\mathbb{Y} = \sum\limits_{i = 1}^d \sqrt{\lambda_i} U_i V_i^{\mathrm{T}}.
\end{gather*}

Теперь перейдём на выборочный язык анализа главных компонент.
Помним, что $\mathbb{Y} = \mathbb{X}^{\mathrm{T}}\in \mathbb{R}^{L\times K}$, где столбцы --- индивиды, строки --- признаки; индивидов $K$, а признаков $L$.\footnote{Визуально: $\mathbb{Y}$ --- горизонтальная матрица, а $\mathbb{X}$ --- вертикальная.} В случае анализа главных компонент
$D_1 = \{1, \ldots, L\}, D_2 = \{1, \ldots, K\}$, $\mu_1(\{i\}) = 1$ --- считающая мера,  $\mu_2(\{i\}) = \frac{1}{K}$  --- вероятностная мера, где $i$ --- номер индивида.
Предполагаем, что $\mathbb{Y}$ --- центрированная по строчкам, то есть среднее по признакам равно 0, и тогда получаем другое разложение:
\begin{gather*}
\mathbb{Y} = \sum\limits_{i = 1}^d \sqrt{\widetilde{\lambda}_i} \widetilde{U}_i \widetilde{V}_i^{\mathrm{T}}.
\end{gather*}

\section{Связь между SVD и АГК. Общее и различия}
Необходимо найти связь между $\widetilde{\lambda}_i$, $\widetilde{U}_i$, $\widetilde{V}_i$ и $\lambda_i$, $U_i$, $V_i$ соответственно.\footnote{В SVD веса 1, а в АГК 1 и $1/K$.} Знаем, что 
\begin{itemize}
\item $U_i$ --- о.н.с с.в. матрицы $\mathbb{Y}\mathbb{Y}^{\mathrm{T}} = \mathbb{X}^{\mathrm{T}} \mathbb{X}$,
\item $\widetilde{U}_i$  --- о.н.с с.в. матрицы $\frac{1}{K}\mathbb{Y}\mathbb{Y}^{\mathrm{T}} = \frac{1}{K}\mathbb{X}^{\mathrm{T}} \mathbb{X}$.
\end{itemize}
То есть получили, что $U_i$ и $\widetilde{U}_i$ совпадают с точностью до коэффициента.\footnote{Если в одном и том же пространстве есть два нормированных вектора: один нормирован с одними весами, другой с другими, то они не должны совпадать. Но здесь у нас понятие нормированности одинаковое за счёт того, что у $U_i$ вес один и тот же --- 1.}

Таким образом, получаем следующие соотношения:
\begin{itemize}
	\item $U_i = \widetilde{U}_i$,
	\item $\lambda_i = K\widetilde{\lambda}_i$,
	\item $V_i = \frac{\widetilde{V}_i}{\sqrt{K}}$.\footnote{Во втором пространстве мера другая, понятие ортонормированности разное.}
\end{itemize}
Также заметим, что
\begin{gather*}
||\mathbb{Y}||_{1,2}^2 = \frac{1}{K} \sum\limits_{ij}  y_{ij}^2 = \frac{||\mathbb{Y}||_F^2}{K}.
\end{gather*}

Рассмотрим теперь отличия сингулярного разложения от анализа главных компонент. 

\begin{enumerate}
	\item Столбцы в матрице $\mathbb{Y}$ неравноправны, то есть SVD полностью симметрично, а АГК нет. Если формально, то получаем, что разные нормы в пространстве признаков.
	\item В АГК предполагается, что признаки центрированы, а индивиды нет.
	\item В АГК рекомендована нормировка признаков.
\end{enumerate}

\textit{Когда нормируем признаки?} Когда признаки измерены в разных шкалах.\footnote{Если что-то измерено в шкале от 0 до 1, а что-то от 0 до 100, то результат АГК будет странным. Всегда первая главная компонента будет там, где сотни.}
\begin{itemize}
	\item Нормируем признаки, если есть, например, данные в сантиметрах и метрах.
	\item Не нормируем признаки, если есть, например, баллы за задачи и хотим, чтобы главная компонента отражала уровень по результатам задач. Пусть есть сложные (от 0 до 10) и простые (от 0 до 5) задачи. Ясно, что получить 2.5 балла за простую задачу и 5 баллов за сложную --- это разные вещи, но если мы нормируем данные, то мы сравняем эти две вещи.
\end{itemize}


\section{Чему АГК соответствует на статистическом языке?}
Перейдём к $\mathbb{X} \in \mathbb{R}^{n \times p}$ ($L \rightarrow p$ --- количество индивидов, $K \rightarrow n$ --- число признаков). Пусть $\mathbb{X}$ --- центрированы. \textit{Берём признак, какую норму нужно рассматривать?} Вероятностную норму. \textit{Что означает характеристика $||X_i||_2^2$ на статистическом языке? }
\begin{gather*}
||X_i||_2^2  = \frac{1}{n} \sum\limits_{j = 1}^n \left((X_i)_j - \overline{X}_i\right)^2 = s^2(X_i) \text{ --- выборочная дисперсия $X_i$.}
\end{gather*}
\textit{Что означает норма матрицы данных $||\mathbb{X}||_{1,2}^2$ на статистическом языке? } Используем верхнюю строчку и предполагаем, что $\mathbb{X}$ центрированы.
\begin{gather*}
||\mathbb{X}||_{1,2}^2 = \sum\limits_{i = 1}^p ||X_i||_2^2 = \sum\limits_{i = 1}^p s^2(X_i) \text{ --- total variance.}
\end{gather*}
Посчитаем норму вектора главных компонент (считаем, что АГК: $\mathbb{Y} = \sum\limits_{i = 1}^d \sqrt{\lambda_i} U_i V_i^{\mathrm{T}}$):
$Z_i = \mathbb{X} U_i = \sqrt{\lambda_i}V_i$, где $Z_i$ --- проекция на $i$-ое направление и $i=1\ldots,n$.

Знаем, что $||Z_i||^2_2 = s^2(Z_i)$. Учитывая, что $V_i$ нормированы, то
\begin{gather*}
||Z_i||^2_2 = s^2(Z_i) = \Arrowvert \mathbb{X} U_i \Arrowvert ^2_2 = \Arrowvert \sqrt{\lambda_i}V_i \Arrowvert^2_2 = \lambda_i.
\end{gather*}

Разложение можем записать следующим образом:
\begin{gather*}
\mathbb{X}^\mathrm{T}=\sum\limits_{i =1}^d \sqrt{\lambda_i} U_i V_i^{\mathrm{T}} = \sum\limits_{i =1}^d F_i V_i^\mathrm{T} = \sum\limits_{i =1}^d U_i Z_i^\mathrm{T},
\end{gather*}
где $F_i=\sqrt{\lambda_i} U_i$ --- вектор $i$-х факторных весов (нагрузок), $Z_i=\sqrt{\lambda_i} V_i$ --- вектор главных компонент.

\section{Вклад главных компонент}

Вычислим норму $\mathbb{Y}=\mathbb{X}^\mathrm{T}$.
\begin{equation*}
\Arrowvert \mathbb{X}^\mathrm{T}\Arrowvert_{1,2}^2 = \sum\limits_{i = 1}^p s^2(X_i) = \sum\limits_{i =1}^d \Arrowvert (\sqrt{\lambda_i}U_i V_i^\mathrm{T})^\mathrm{T} \Arrowvert_2^2 = \sum\limits_{i =1}^d \lambda_i = \sum\limits_{i =1}^d s^2(Z_i).
\end{equation*}

Получилось, что total variance не меняется при переходе к новым признакам (при повороте норма векторов не меняется).

$\frac{\lambda_j}{\sum\limits_{i=1}^d \lambda_i}$ --- \textit{вклад $j$-ой главной компоненты в общую дисперсию}.\footnote{В числителе --- дисперсия нового признака, в знаменателе --- общая дисперсия.}

$s^2(X_i)$ --- информативность $i$-ого признака, $s^2(Z_i)$ --- информативность $i$-ой главной компоненты. Таким образом, чем больше разброс, тем больше эта характеристика информативна. Первая главная компонента имеет наибольшую норму, поэтому эта компонента самая информативная.

Напомним, что $Z_i=\mathbb{X}U_i$, то есть $Z_i$ --- линейная комбинация $X_j$ с коэффициентами, взятыми из $U_i$. Таким образом, \textit{главные компоненты} --- это ортогональные между собой линейные комбинации исходных признаков, обладающие свойством оптимальности.

Возможны случаи:
\begin{enumerate}
	\item Матрица $\mathbb{X}$ --- центрирована. Тогда $U_i$ --- это собственные векторы матрицы $\frac{1}{n}\mathbb{Y}\mathbb{Y}^\mathrm{T} = \frac{1}{n}\mathbb{X}^\mathrm{T}\mathbb{X}$ (выборочная ковариационная матрица).
	
	\item Матрица $\mathbb{X}$ --- центрирована и нормирована. Тогда $U_i$ --- собственные векторы матрицы $\frac{1}{n}\mathbb{Y}\mathbb{Y}^\mathrm{T} = \frac{1}{n}\mathbb{X}^\mathrm{T}\mathbb{X}$ (выборочная корреляционная матрица).
\end{enumerate}

\section{АГК с точки зрения построения базиса в пространстве индивидов и в пространстве признаков}

Анализ главных компонент на выборочном языке: $\mathbb{X}\in\mathbb{R}^{n\times p},~U_i\in\mathbb{R}^p,~V_i\in\mathbb{R}^n$.
\begin{equation*}
\mathbb{X}^\mathrm{T}=\sum\limits_{i =1}^d \sqrt{\lambda_i} U_i V_i^{\mathrm{T}} = \sum\limits_{i =1}^d F_i V_i^\mathrm{T} = \sum\limits_{i =1}^d U_i Z_i^\mathrm{T},
\end{equation*}

$F_i=\sqrt{\lambda_i} U_i$ --- вектор $i$-х факторных весов (нагрузок), 

$Z_i=\sqrt{\lambda_i} V_i$ --- вектор главных компонент.

\begin{enumerate}
	\item $\mathbb{X}^\mathrm{T}=\sum\limits_{i =1}^d F_i V_i^\mathrm{T}$, где $\{V_i\}_{i=1}^d$ --- ортонормированный базис в пространстве признаков.
	
	Пусть $\mathbb{F}=\{f_{ij}\}_{i=1,j=1}^{p,d}=[F_1:\ldots,F_d]$. Тогда
	\begin{equation*}
	f_{ij}=\underbrace{\langle X_i, V_j \rangle_2}_{\substack{j\text{-я коорд. } i\text{-ого признака} \\ \text{в базисе новых признаков}}} = \begin{cases}
	\text{Cov} (X_i,V_j), \text{ если АГК по ковариац. матр.} \\
	\rho(X_i,V_j)=\rho(X_i,Z_j), \text{ если АГК по корреляц. матр.} 
	\end{cases} 
	\end{equation*} 
	
	
	\item $\mathbb{X}^\mathrm{T}=\sum\limits_{i =1}^d U_i Z_i^\mathrm{T}$, где $\{U_i\}_{i=1}^d$ --- ортонормированный базис в пространстве индивидов.
	
	Пусть $\mathbb{Z}=\{z_{ij}\}_{i=1,j=1}^{n,d}=[Z_1:\ldots,Z_d]$. Тогда
	\begin{equation*}
	z_{ij}=\underbrace{\langle \mathbf{x}_i, U_j \rangle_1}_{\substack{j\text{-я коорд. } i\text{-ого индивида} \\ \text{в новом базисе}}}. 
	\end{equation*} 
	Так как индивиды не центрированы и не нормированы, это равенство продолжить по аналогии с предыдущим пунктом не можем. Но можем выписать следующее. Пусть $\alpha(\mathbf{x}_i,U_j)$ --- угол между $\mathbf{x}_i$ и $U_j$. Тогда
	\begin{equation*}
	\cos(\alpha(\mathbf{x}_i,U_j)) = \frac{\langle \mathbf{x}_i, U_j \rangle_1}{\Arrowvert \mathbf{x}_i \Arrowvert \Arrowvert U_j \Arrowvert} = \frac{\langle \mathbf{x}_i, U_j \rangle_1}{\Arrowvert \mathbf{x}_i \Arrowvert}.
	\end{equation*}
	
\end{enumerate}
Эти два пункта помогают ответить на вопрос --- \textit{как выявить индивидов, которые плохо описываются плоскостью первых двух компонент?}

Чтобы посчитать, как хорошо индивид описывается плоскостью, надо посчитать косинус угла между плоскостью и индивидом. Очевидно, что индивиды, перпендикулярные плоскости, плохо описываются этой плоскостью. Если есть ортогональный базис, то верно следующее: $\cos^2$(угла между вектором и проекцией на плоскость)=$\cos^2$(угла между вектором и 1-ым элементом базиса)$+\cos^2$(угла между вектором и 2-ым элементом базиса).

Пусть $Y_i\in\mathbb{R}^p$ --- индивид. Тогда косинус угла между ним и плоскостью первых двух главных компонент:
\begin{equation*}
\cos^2(\alpha(Y_i,\text{span}(U_1,U_2)))=\frac{\overbrace{\langle Y_i,U_1\rangle^2}^{z_{i1}^2}}{\Arrowvert Y_i \Arrowvert^2} + \frac{\overbrace{\langle Y_i,U_2\rangle^2}^{z_{i2}^2}}{\Arrowvert Y_i \Arrowvert^2} = \frac{z_{i1}^2}{\sum\limits_{j=1}^d z_{ij}^2} + \frac{z_{i2}^2}{\sum\limits_{j=1}^d z_{ij}^2}.
\end{equation*}

Мы получили, что, складывая квадраты нормированных строк $\mathbb{Z}$, мы можем получать квадраты косинусов углов между индивидами и плоскостью. Пусть признаки стандартизованы. Аналогично можем получить, что 
\begin{equation*}
\cos^2(\alpha(X_j,\text{span}(V_1,V_2)))=f_{j1}^2+f_{j2}^2.
\end{equation*}

Если все центрировано, то косинус можно назвать корреляцией (формулы для косинуса и коэффициента корреляции совпадут). Тогда получим, что множественный коэффициент корреляции равен сумме квадратов обычных корреляций, то есть 
\begin{equation*}
R^2(X_j;V_1,V_2) = \rho^2(X_j,V_1)+\rho(X_j,V_2).
\end{equation*}

\begin{remark} $\mathbb{U}=[U_1:\ldots:U_d],~\mathbb{F}=[F_1:\ldots:F_d] $
	\begin{enumerate}
		\item $\sum\limits_{i=1}^p u_{ij}^2=\Arrowvert U_j \Arrowvert^2 =1$ 
		\item $\sum\limits_{j=1}^d u_{ij}^2 =1$ 
		\item $\sum\limits_{i=1}^p f_{ij}^2 =\Arrowvert F_j \Arrowvert^2 =\lambda_j$ 
		\item $\sum\limits_{j=1}^d f_{ij}^2 =\sum\limits_{j=1}^d\langle X_i,V_j\rangle_2^2=\Arrowvert X_i \Arrowvert_2^2 = \begin{cases}
		s^2(X_i), \text{ по ковариац. матр.}\\
		1, \text{ по корреляц. матр.}
		\end{cases}$ 		
	\end{enumerate}
\end{remark}

	Скалярное произведение	$\langle X_i,V_1\rangle_2^2$ характеризует то, насколько 1-ый новый признак описывает исходный. Если рассмотреть $\langle X_i,V_1\rangle_2^2 + \langle X_i,V_2\rangle_2^2$, то это то, насколько первых два новых признака описывают старый.

\section{Интерпретация главных компонент. Смысл первой главной компоненты в случае положительных ковариаций}

\begin{theorem}[Перрона-Фробениуса]
	Пусть $\mathbb{A}$ --- симметричная, неотрицательно определенная матрица, ее элементы положительны. Тогда все компоненты ее первого собственного вектора $U_1$ будут одного знака.
\end{theorem}

Таким образом, \textit{если все корреляции (ковариации) положительны, то все компоненты $U_1$ одного знака.} Это определяет смысл первой главной компоненты (в случае положительных ковариаций). Тогда первая главная компонента является линейной комбинацией старых признаков с положительными коэффициентами. Это можно проинтерпретировать как некий общий уровень чего-либо (например, общий уровень ученика, если признаки --- оценки в школе). Остальные главные компоненты можно также интерпретировать исходя из коэффициентов перед старыми признаками.\footnote{Напомним, что коэффициентами линейной комбинации являются элементы векторов $U_i$.}

\section{Выбор числа главных компонент}

Приведем некоторые варианты выбора числа главных компонент.

\begin{enumerate}
\item Задается процент $P$ и берется $\tau$ компонент: 
\begin{equation*}
\frac{\sum\limits_{i=1}^\tau \lambda_i}{\sum\limits_{i=1}^d \lambda_i} > P\%,
\end{equation*}
то есть чтобы компоненты несли в себе не менее $P\%$ информации.

\item Правило Кайзера.
Выбираются главные компоненты, информативность которых больше средней информативности:
\begin{equation*}
i:~~\lambda_i>\frac{\sum\limits_{i=1}^p s^2(X_i)}{p} = \frac{\sum\limits_{i=1}^d \lambda_i}{p}.
\end{equation*}
Если АГК по корреляционной матрице, то $i:~~\lambda_i>1$.

\item Правило сломанной трости. Пусть $\mu_i = \frac{\lambda_i}{\sum\limits_{i=1}^d \lambda_i}$. Числа $\mu_i$ делят отрезок $[0,1]$ на неравные части. Получаем разбиение $0<\mu_1<\mu_2<\ldots<\mu_{d-1}<1$. Выбирается компонента, длина которой больше средней длины кусочка случайно сломанной трости.

\item Правило каменистой осыпи. Строим график упорядоченных по убыванию собственных чисел (scree plot). С какого-то момента собственные числа начинают медленно меняться. Берем компоненты до этого момента, то есть пока собственные числа отличаются друг от друга.

	\begin{center}
	\begin{minipage}{0.51\linewidth}
		\centering
		\includegraphics[width=250pt, height=170pt]{Scree}
		\captionof{figure}{Scree plot}
	\end{minipage}
\end{center}

\item Интерпретируем столько компонент, сколько можем.
\end{enumerate}

\section{Оптимизация в АГК в терминах ковариационных матриц}

\begin{proposition}
	$\mathbb{Y}=\mathbb{X}^\mathrm{T}$.
	
	 Задача $\Arrowvert \mathbb{Y}-\widetilde{\mathbb{Y}}\Arrowvert \to \min \limits_{\widetilde{\mathbb{Y}}:~\text{rank}\widetilde{\mathbb{Y}}\le r}$\footnote{решение этой задачи --- сумма собственных троек, соответствующих первым $r$ главным компонентам.} эквивалентна задаче $\Arrowvert \mathbb{Y}\mathbb{Y}^\mathrm{T}-\widetilde{\mathbb{Y}}\widetilde{\mathbb{Y}}^\mathrm{T}\Arrowvert \to \min \limits_{\widetilde{\mathbb{Y}}:~\text{rank}\widetilde{\mathbb{Y}}\le r}$.
\end{proposition}

Если матрица центрирована, то задача $\Arrowvert \mathbb{Y}\mathbb{Y}^\mathrm{T}-\widetilde{\mathbb{Y}}\widetilde{\mathbb{Y}}^\mathrm{T}\Arrowvert \to \min \limits_{\widetilde{\mathbb{Y}}:~\text{rank}\widetilde{\mathbb{Y}}\le r}$ эквивалентна следующей задаче:
\begin{equation*}
\Arrowvert \mathbb{S}-\widetilde{\mathbb{S}}\Arrowvert \to \min \limits_{\widetilde{\mathbb{S}}:~\text{rank}\widetilde{\mathbb{S}}\le r},
\end{equation*}
где $\mathbb{S}$ --- ковариационная матрица для наших данных, а $\widetilde{\mathbb{S}}$ --- какая-то ковариационная матрица (симметричная, неотрицательно определенная). 




\end{document}